# Chicago Air Quality Monitoring System â€” Version 2
[Project Link](https://chicagoairqualityprediction.vercel.app/)
(It can take upto 40s to connect to mongodb since it is deployed on free instance of vercel)
This repository contains the updated, **end-to-end implementation** of a Chicago air quality monitoring and prediction system.  
The project was originally built as a frontend-only visualization tool (V1).  
**Version 2** expands it into a full **data engineering and machine learning pipeline**.

---

## ğŸ“Š Overview of V1 (Previous Version)

**Repository:** [https://github.com/Tilak2203/Chicago-air-quality-vercel](https://github.com/Tilak2203/Chicago-air-quality-vercel)

The earlier version (V1) consisted of:

- A React application
- Direct API calls to fetch PM2.5 readings
- Basic data visualization
- No automated data pipeline
- No backend
- No stored history
- No machine learning predictions

### Limitations of V1:

- Data was fetched only at runtime
- No long-term storage or historical dataset
- No ETL or cleaning
- No prediction capability
- The entire system existed only on the frontend

---

## ğŸš€ Overview of V2 (Current Version)

Version 2 transitions the project to a **proper end-to-end pipeline**.  
It separates ingestion, processing, storage, prediction, API serving, and frontend visualization into **independent components**.

### Key Changes:

- âœ… Added **Airflow ETL pipeline** for scheduled data ingestion and cleaning
- âœ… Introduced **MongoDB Atlas** as the primary database
- âœ… Added a **Python backend** to serve data and predictions through REST APIs
- âœ… Implemented a **machine learning model (RandomForest)** for PM2.5 forecasting
- âœ… Rebuilt the frontend to use backend APIs instead of direct external calls
- âœ… Centralized timestamp handling and preprocessing
- âœ… Organized the project into a **modular, production-style structure**

---

## ğŸ“ Project Structure

```
Chicago-Air-Quality-Monitoring-v2/
â”‚
â”œâ”€â”€ airflow-docker/                 # Airflow ETL pipeline (Dockerized)
â”‚   â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ docker-compose.yaml
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ Chicago-air-quality-render/     # Python backend (API + ML)
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ mongodb.py
â”‚   â”œâ”€â”€ predict.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â”œâ”€â”€ model.pkl
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ Chicago-air-quality-vercel/     # React frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ public/
â”‚   â””â”€â”€ package.json
â”‚
â””â”€â”€ data/                           # Local CSV storage (optional)
```

---

## ğŸ—ï¸ Architecture Summary

The system now follows a **straightforward pipeline**:

1. **Airflow** fetches PM2.5 data on a schedule, cleans it, normalizes timestamps, removes duplicates, and loads it into MongoDB.

2. **MongoDB Atlas** stores historical and latest readings.

3. **The Python backend** exposes REST endpoints for:
   - All cleaned readings
   - Prediction results
   - Prediction history
   - Model performance metrics

4. A **RandomForest model** provides next-hour PM2.5 predictions.

5. **The React frontend** consumes backend APIs, visualizes trends, allows date filtering, and displays predictions.

This mirrors a standard data engineering workflow:  
**ingest â†’ clean â†’ store â†’ predict â†’ serve â†’ visualize**

---

## ğŸ› ï¸ Running the System Locally

### 1. Start Airflow

```bash
cd airflow-docker
docker compose up airflow-init
docker compose up -d
```

### 2. Start Backend

```bash
cd Chicago-air-quality-render
pip install -r requirements.txt
python app.py
```

**Backend runs at:** `http://localhost:5000`

### 3. Start Frontend

```bash
cd Chicago-air-quality-vercel
npm install
npm start
```

**Frontend runs at:** `http://localhost:3000`

---

## ğŸ“Š Version Comparison

| Feature | V1 | V2 |
|---------|----|----|
| **Frontend** | Yes | Yes (updated) |
| **ETL Pipeline** | No | Yes (Airflow) |
| **Database** | No | Yes (MongoDB Atlas) |
| **Backend API** | No | Yes |
| **Machine Learning** | No | Yes |
| **Historical Storage** | No | Yes |
| **Automated Scheduling** | No | Yes |
| **Timestamp Cleaning** | Basic | Robust |

---

## ğŸ¯ Purpose of V2

This version provides a more **realistic demonstration** of data engineering and machine learning concepts:

- âœ”ï¸ Automated, repeatable ingestion
- âœ”ï¸ Centralized cloud database
- âœ”ï¸ A reusable API layer
- âœ”ï¸ Prediction workflows
- âœ”ï¸ A clean separation between ETL, backend, and frontend

The system is **maintainable, extendable, and reflects real-world architecture**.

---

## ğŸ“ License

This project is open-source and available for educational purposes.

---

## ğŸ‘¤ Author

**Tilak2203**
